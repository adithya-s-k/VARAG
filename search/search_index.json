{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc41\ufe0f\ud83d\udc41\ufe0f VARAG","text":"<p>Vision Augmented Retrieval and Generation</p> VARAG (Vision-Augmented Retrieval and Generation) is a vision-first RAG engine that emphasizes vision-based retrieval techniques. It enhances traditional Retrieval-Augmented Generation (RAG) systems by integrating both visual and textual data through Vision-Language models. <p> </p>"},{"location":"#supported-retrieval-techniques","title":"Supported Retrieval Techniques","text":"<p>VARAG supports a wide range of retrieval techniques, optimized for different use cases, including text, image, and multimodal document retrieval. Below are the primary techniques supported:</p> Simple RAG (with OCR) Simple RAG (Retrieval-Augmented Generation) is an efficient and straightforward approach to extracting text from documents and feeding it into a retrieval pipeline. VARAG incorporates Optical Character Recognition (OCR) through Docling, making it possible to process and index scanned PDFs or images. After the text is extracted and indexed, queries can be matched to relevant passages in the document, providing a strong foundation for generating responses that are grounded in the extracted information. This technique is ideal for text-heavy documents like scanned books, contracts, and research papers, and can be paired with Large Language Models (LLMs) to produce contextually aware outputs.   Vision RAG Vision RAG extends traditional RAG techniques by incorporating the retrieval of visual information, bridging the gap between text and images. Using a powerful cross-modal embedding model like JinaCLIP (a variant of CLIP developed by Jina AI), both text and images are encoded into a shared vector space. This allows for similarity searches across different modalities, meaning that images can be queried alongside text. Vision RAG is particularly useful for document analysis tasks where visual components (e.g., figures, diagrams, images) are as important as the textual content. It\u2019s also effective for tasks like image captioning or generating product descriptions where understanding and correlating text with visual elements is critical.   ColPali RAG ColPali RAG represents a cutting-edge approach that simplifies the traditional retrieval pipeline by directly embedding document pages as images rather than converting them into text. This method leverages PaliGemma, a Vision Language Model (VLM) from the Google Z\u00fcrich team, which encodes entire document pages into vector embeddings, treating the page layout and visual elements as part of the retrieval process. Using a late interaction mechanism inspired by ColBERT (Column BERT), ColPali RAG enhances retrieval by enabling token-level matching between user queries and document patches. This approach ensures high retrieval accuracy while also maintaining reasonable indexing and querying speeds. It is particularly beneficial for documents rich in visuals, such as infographics, tables, and complex layouts, where conventional text-based retrieval methods struggle.   Hybrid ColPali RAG Hybrid ColPali RAG further enhances retrieval performance by combining the strengths of both image embeddings and ColPali\u2019s late interaction mechanism. In this approach, the system first performs a coarse retrieval step using image embeddings (e.g., from a model like JinaCLIP) to retrieve the top-k relevant document pages. Then, in a second pass, the system re-ranks these k pages using the ColPali late interaction mechanism to identify the final set of most relevant pages based on both visual and textual information. This hybrid approach is particularly useful when documents contain a mixture of complex visuals and detailed text, allowing the system to leverage both content types for highly accurate document retrieval."},{"location":"#getting-started-with-varag","title":"\ud83d\ude80 Getting Started with VARAG","text":"<p>Follow these steps to set up VARAG:</p>"},{"location":"#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/adithya-s-k/VARAG\ncd VARAG\n</code></pre>"},{"location":"#2-set-up-environment","title":"2. Set Up Environment","text":"<p>Create and activate a virtual environment using Conda:</p> <pre><code>conda create -n varag-venv python=3.10\nconda activate varag-venv\n</code></pre>"},{"location":"#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Install the required packages using pip:</p> <pre><code>pip install -e .\n\n# or \n\npoetry install\n</code></pre> <p>To install OCR dependencies:</p> <pre><code>pip install -e .[\"ocr\"]\n</code></pre>"},{"location":"#try-out-varag","title":"Try Out VARAG","text":"<p>Explore VARAG with our interactive playground! It lets you seamlessly compare various RAG (Retrieval-Augmented Generation) solutions, from data ingestion to retrieval.</p> <p></p> <p>You can run it locally or on Google Colab: <pre><code>python demo.py --share\n</code></pre></p> <p>This makes it easy to test and experiment with different approaches in real-time.</p>"},{"location":"#how-varag-is-structured","title":"How VARAG is structured","text":"<p>Each RAG technique is structured as a class, abstracting all components and offering the following methods:</p> <pre><code>from varag.rag import {{RAGTechnique}}\n\nragTechnique = RAGTechnique()\n\nragTechnique.index(\n  \"/path_to_data_source\",\n  other_relevant_data\n)\n\nresults = ragTechnique.search(\"query\", top_k=5)\n\n# These results can be passed into the LLM / VLM of your choice\n</code></pre>"},{"location":"#why-abstract-so-much","title":"Why Abstract So Much?","text":"<p>I initially set out to rapidly test and evaluate different Vision-based RAG (Retrieval-Augmented Generation) systems to determine which one best fits my use case. I wasn\u2019t aiming to create a framework or library, but it naturally evolved into one. </p> <p>The abstraction is designed to simplify the process of experimenting with different RAG paradigms without complicating compatibility between components. To keep things straightforward, LanceDB was chosen as the vector store due to its ease of use and high customizability.</p> <p>This paradigm is inspired by the Byaldi repo by Answer.ai.</p>"},{"location":"#techniques-and-notebooks","title":"Techniques and Notebooks","text":"Technique Notebook Demo Simple RAG simpleRAG.py Vision RAG visionDemo.py Colpali RAG colpaliDemo.py Hybrid Colpali RAG hybridColpaliDemo.py"},{"location":"#explanation","title":"Explanation:","text":"<ul> <li>Technique: This column lists the different techniques implemented for Retrieval-Augmented Generation (RAG).</li> <li>Notebook: Colab links with the \"Open In Colab\" button for interactive exploration of each technique.</li> <li>Demo: Links to the corresponding demo scripts in the repository that can be executed locally.</li> </ul>"},{"location":"#contributing","title":"\ud83d\udee0\ufe0f Contributing","text":"<p>Contributions to VARAG are highly encouraged! Whether it's code improvements, bug fixes, or feature enhancements, feel free to contribute to the project repository. Please adhere to the contribution guidelines outlined in the repository for smooth collaboration.</p>"},{"location":"#license","title":"\ud83d\udcdc License","text":"<p>VARAG is licensed under the MIT License, granting you the freedom to use, modify, and distribute the code in accordance with the terms of the license.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>We extend our sincere appreciation to the following projects and their developers:</p> <ul> <li>Docling - For PDF text extraction (OCR) and text extraction.</li> <li>LanceDB - For vector database functionality.</li> </ul> <p>This project also draws inspiration from the following repositories:</p> <ul> <li>Byaldi</li> <li>RAGatouille</li> </ul> <p>For the implementation of Colpali, we referred to the following blogs and codebases:</p> <ul> <li>Vision Retrieval by Kyryl</li> <li>Vision Retrieval by AyushExel</li> <li>The Rise of Vision-Driven Document Retrieval for RAG</li> </ul> <p>We would also like to acknowledge the authors of the ColPali paper, which significantly influenced our work:</p> <pre><code>@misc{faysse2024colpaliefficientdocumentretrieval,\n      title={ColPali: Efficient Document Retrieval with Vision Language Models}, \n      author={Manuel Faysse and Hugues Sibille and Tony Wu and Bilel Omrani and Gautier Viaud and C\u00e9line Hudelot and Pierre Colombo},\n      year={2024},\n      eprint={2407.01449},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2407.01449}, \n}\n</code></pre>"},{"location":"colpaliRAG/","title":"Colpali RAG","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/adithya-s-k/VARAG\n%cd VARAG\n%pwd\n</pre> !git clone https://github.com/adithya-s-k/VARAG %cd VARAG %pwd In\u00a0[\u00a0]: Copied! <pre>!apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils\n</pre> !apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils In\u00a0[\u00a0]: Copied! <pre>%pip install -e .\n</pre> %pip install -e . In\u00a0[\u00a0]: Copied! <pre>from sentence_transformers import SentenceTransformer\nfrom varag.rag import ColpaliRAG\nfrom varag.llms import OpenAI\nfrom varag.utils import get_model_colpali\nimport lancedb\nimport os\nfrom dotenv import load_dotenv\n\nos.environ[\"OPENAI_API_KEY\"] = \"api-key\"\n\nload_dotenv()\n</pre> from sentence_transformers import SentenceTransformer from varag.rag import ColpaliRAG from varag.llms import OpenAI from varag.utils import get_model_colpali import lancedb import os from dotenv import load_dotenv  os.environ[\"OPENAI_API_KEY\"] = \"api-key\"  load_dotenv() In\u00a0[\u00a0]: Copied! <pre>shared_db = lancedb.connect(\"~/shared_rag_db\")\n\nmodel, processor = get_model_colpali(\"vidore/colpali-v1.2\")\n\ncolpali_rag = ColpaliRAG(\n    colpali_model=model,\n    colpali_processor=processor,\n    db=shared_db,\n    table_name=\"colpaliDemo\",\n)\n\nvlm = OpenAI()\n</pre> shared_db = lancedb.connect(\"~/shared_rag_db\")  model, processor = get_model_colpali(\"vidore/colpali-v1.2\")  colpali_rag = ColpaliRAG(     colpali_model=model,     colpali_processor=processor,     db=shared_db,     table_name=\"colpaliDemo\", )  vlm = OpenAI() In\u00a0[\u00a0]: Copied! <pre>colpali_rag.index(\n        \"./examples/data\", \n        overwrite=False, \n        recursive=False, \n        verbose=True\n    )\n</pre> colpali_rag.index(         \"./examples/data\",          overwrite=False,          recursive=False,          verbose=True     ) In\u00a0[\u00a0]: Copied! <pre>query = \"What is Colpali\"\nnum_results = 5\n\nresults = colpali_rag.search(query, k=5)\n\nimages = [result[\"image\"] for result in results]\n\n# Display the images\nfor i, img in enumerate(images, 1):\n    print(f\"Image {i}:\")\n    display(img)\n</pre> query = \"What is Colpali\" num_results = 5  results = colpali_rag.search(query, k=5)  images = [result[\"image\"] for result in results]  # Display the images for i, img in enumerate(images, 1):     print(f\"Image {i}:\")     display(img) In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, Markdown\n\n\nresponse = vlm.query(query, images, max_tokens=1000)\n\n\ndisplay(Markdown(response))\n</pre> from IPython.display import display, Markdown   response = vlm.query(query, images, max_tokens=1000)   display(Markdown(response)) In\u00a0[\u00a0]: Copied! <pre>%cd examples\n!python colpaliDemo.py --share\n</pre> %cd examples !python colpaliDemo.py --share"},{"location":"colpaliRAG/#colpali-rag-using-varag","title":"Colpali RAG using VARAG\u00b6","text":"<p>Requirement to RUN this notebook - Min T4 GPU</p>"},{"location":"colpaliRAG/#run-gradio-demo","title":"Run Gradio Demo\u00b6","text":""},{"location":"demo/","title":"Demo","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/adithya-s-k/VARAG\n%cd VARAG\n%pwd\n</pre> !git clone https://github.com/adithya-s-k/VARAG %cd VARAG %pwd In\u00a0[\u00a0]: Copied! <pre>!apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils\n</pre> !apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils In\u00a0[\u00a0]: Copied! <pre>%pip install -e .\n\n## We will be using Docling for OCR\n%pip install docling\n</pre> %pip install -e .  ## We will be using Docling for OCR %pip install docling In\u00a0[\u00a0]: Copied! <pre>!python demo.py --share\n</pre> !python demo.py --share"},{"location":"demo/#varag-plaground","title":"VARAG Plaground\u00b6","text":"<p>Explore VARAG with our interactive playground! It lets you seamlessly compare various RAG (Retrieval-Augmented Generation) solutions, from data ingestion to retrieval.</p> <p></p> <p>This makes it easy to test and experiment with different approaches in real-time.</p>"},{"location":"demo/#run-gradio","title":"Run Gradio\u00b6","text":""},{"location":"hybridColpaliRAG/","title":"Hybrid Colpali RAG","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/adithya-s-k/VARAG\n%cd VARAG\n%pwd\n</pre> !git clone https://github.com/adithya-s-k/VARAG %cd VARAG %pwd In\u00a0[\u00a0]: Copied! <pre>!apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils\n</pre> !apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils In\u00a0[\u00a0]: Copied! <pre>%pip install -e .\n</pre> %pip install -e . In\u00a0[\u00a0]: Copied! <pre>from sentence_transformers import SentenceTransformer\nfrom varag.rag import HybridColpaliRAG\nfrom varag.llms import OpenAI\nfrom varag.utils import get_model_colpali\nimport lancedb\nimport os\nfrom dotenv import load_dotenv\n\nos.environ[\"OPENAI_API_KEY\"] = \"api-key\"\n\nload_dotenv()\n</pre> from sentence_transformers import SentenceTransformer from varag.rag import HybridColpaliRAG from varag.llms import OpenAI from varag.utils import get_model_colpali import lancedb import os from dotenv import load_dotenv  os.environ[\"OPENAI_API_KEY\"] = \"api-key\"  load_dotenv() In\u00a0[\u00a0]: Copied! <pre>shared_db = lancedb.connect(\"~/shared_rag_db\")\n\nmodel, processor = get_model_colpali(\"vidore/colpali-v1.2\")\nembedding_model = SentenceTransformer(\"jinaai/jina-clip-v1\", trust_remote_code=True)\n\ncolpali_hybrid_rag = HybridColpaliRAG(\n    colpali_model=model,\n    colpali_processor=processor,\n    db=shared_db,\n    image_embedding_model=embedding_model,\n    table_name=\"hybridColpaliDemo\",\n)\n\nvlm = OpenAI()\n</pre> shared_db = lancedb.connect(\"~/shared_rag_db\")  model, processor = get_model_colpali(\"vidore/colpali-v1.2\") embedding_model = SentenceTransformer(\"jinaai/jina-clip-v1\", trust_remote_code=True)  colpali_hybrid_rag = HybridColpaliRAG(     colpali_model=model,     colpali_processor=processor,     db=shared_db,     image_embedding_model=embedding_model,     table_name=\"hybridColpaliDemo\", )  vlm = OpenAI() In\u00a0[\u00a0]: Copied! <pre>colpali_hybrid_rag.index(\n        \"./examples/data\", \n        overwrite=False, \n        recursive=False, \n        verbose=True\n    )\n</pre> colpali_hybrid_rag.index(         \"./examples/data\",          overwrite=False,          recursive=False,          verbose=True     ) In\u00a0[\u00a0]: Copied! <pre>query = \"What is Colpali\"\nnum_results = 5\n\nresults = colpali_hybrid_rag.search(query, k=5)\n\nimages = [result[\"image\"] for result in results]\n\n# Display the images\nfor i, img in enumerate(images, 1):\n    print(f\"Image {i}:\")\n    display(img)\n</pre> query = \"What is Colpali\" num_results = 5  results = colpali_hybrid_rag.search(query, k=5)  images = [result[\"image\"] for result in results]  # Display the images for i, img in enumerate(images, 1):     print(f\"Image {i}:\")     display(img) In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, Markdown\n\n\nresponse = vlm.query(query, images, max_tokens=1000)\n\n\ndisplay(Markdown(response))\n</pre> from IPython.display import display, Markdown   response = vlm.query(query, images, max_tokens=1000)   display(Markdown(response)) In\u00a0[\u00a0]: Copied! <pre>%cd examples\n!python hybridColpaliDemo.py --share\n</pre> %cd examples !python hybridColpaliDemo.py --share"},{"location":"hybridColpaliRAG/#hybrid-colpali-rag-using-varag","title":"Hybrid Colpali RAG using VARAG\u00b6","text":"<p>Requirement to RUN this notebook - Min T4 GPU</p>"},{"location":"hybridColpaliRAG/#run-gradio-demo","title":"Run Gradio Demo\u00b6","text":""},{"location":"simpleRAG/","title":"Simple RAG","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/adithya-s-k/VARAG\n%cd VARAG\n%pwd\n</pre> !git clone https://github.com/adithya-s-k/VARAG %cd VARAG %pwd In\u00a0[\u00a0]: Copied! <pre>!apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils\n</pre> !apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils In\u00a0[\u00a0]: Copied! <pre>%pip install -e .\n\n## We will be using Docling for OCR\n%pip install docling\n</pre> %pip install -e .  ## We will be using Docling for OCR %pip install docling In\u00a0[1]: Copied! <pre>from sentence_transformers import SentenceTransformer\nfrom varag.rag import SimpleRAG\nfrom varag.llms import OpenAI\nfrom varag.llms import LiteLLM\nfrom varag.chunking import FixedTokenChunker\nimport lancedb\nimport os\nfrom dotenv import load_dotenv\n\n# os.environ[\"OPENAI_API_KEY\"] = \"api-key\"\n\nload_dotenv()\n</pre> from sentence_transformers import SentenceTransformer from varag.rag import SimpleRAG from varag.llms import OpenAI from varag.llms import LiteLLM from varag.chunking import FixedTokenChunker import lancedb import os from dotenv import load_dotenv  # os.environ[\"OPENAI_API_KEY\"] = \"api-key\"  load_dotenv() <pre>/home/adithya/miniconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n  from tqdm.autonotebook import tqdm, trange\n</pre> <pre>\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 2\n      1 from sentence_transformers import SentenceTransformer\n----&gt; 2 from varag.rag import SimpleRAG\n      3 from varag.llms import OpenAI\n      4 from varag.llms import LiteLLM\n\nFile ~/workspace/VARAG/varag/rag/__init__.py:1\n----&gt; 1 from ._simpleRAG import SimpleRAG\n      2 from ._colpaliRAG import ColpaliRAG\n      3 from ._hybridColpaliRAG import HybridColpaliRAG\n\nFile ~/workspace/VARAG/varag/rag/_simpleRAG.py:16\n     14 from openai import OpenAI\n     15 from dotenv import load_dotenv\n---&gt; 16 from varag.chunking import BaseChunker, FixedTokenChunker\n     17 from sklearn.metrics import precision_score, recall_score, f1_score\n     18 import pandas as pd\n\nModuleNotFoundError: No module named 'varag.chunking'</pre> In\u00a0[\u00a0]: Copied! <pre>OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</pre> OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") In\u00a0[\u00a0]: Copied! <pre># Initialize OpenAI LLM\n# llm = LiteLLM(model=\"gpt-4o-mini\" , is_vision_required=True, api_key=OPENAI_API_KEY) \nllm = LiteLLM(model=\"gpt-3.5-turbo\" , is_vision_required=True, api_key=OPENAI_API_KEY)\n</pre> # Initialize OpenAI LLM # llm = LiteLLM(model=\"gpt-4o-mini\" , is_vision_required=True, api_key=OPENAI_API_KEY)  llm = LiteLLM(model=\"gpt-3.5-turbo\" , is_vision_required=True, api_key=OPENAI_API_KEY)  In\u00a0[\u00a0]: Copied! <pre>response = llm.query(query=\"What is you name?\" , )\nprint(response)\n</pre> response = llm.query(query=\"What is you name?\" , ) print(response) In\u00a0[\u00a0]: Copied! <pre>embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", trust_remote_code=True)\n# embedding_model = SentenceTransformer(\"BAAI/bge-base-en\", trust_remote_code=True)\n# embedding_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", trust_remote_code=True)\n# embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", trust_remote_code=True)\n\n# Initialize shared database\nshared_db = lancedb.connect(\"~/shared_rag_db\")\n\n# Initialize TextRAG with shared database\ntext_rag = SimpleRAG(\n    text_embedding_model=embedding_model,\n    db=shared_db,\n    table_name=\"textDemo\",\n)\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n# Initialize OpenAI LLM\nllm = LiteLLM(model=\"gpt-4o-mini\" , is_vision_required=True, api_key=OPENAI_API_KEY)\n</pre> embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", trust_remote_code=True) # embedding_model = SentenceTransformer(\"BAAI/bge-base-en\", trust_remote_code=True) # embedding_model = SentenceTransformer(\"BAAI/bge-large-en-v1.5\", trust_remote_code=True) # embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\", trust_remote_code=True)  # Initialize shared database shared_db = lancedb.connect(\"~/shared_rag_db\")  # Initialize TextRAG with shared database text_rag = SimpleRAG(     text_embedding_model=embedding_model,     db=shared_db,     table_name=\"textDemo\", )  OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") # Initialize OpenAI LLM llm = LiteLLM(model=\"gpt-4o-mini\" , is_vision_required=True, api_key=OPENAI_API_KEY) In\u00a0[\u00a0]: Copied! <pre>text_rag.index(\n        \"./examples/data\",\n        recursive=False,\n        chunking_strategy=FixedTokenChunker(chunk_size=1000),\n        metadata={\"source\": \"gradio_upload\"},\n        overwrite=True,\n        verbose=True,\n        ocr=True,\n    )\n</pre> text_rag.index(         \"./examples/data\",         recursive=False,         chunking_strategy=FixedTokenChunker(chunk_size=1000),         metadata={\"source\": \"gradio_upload\"},         overwrite=True,         verbose=True,         ocr=True,     ) In\u00a0[\u00a0]: Copied! <pre>query = \"what is colpali ?\"\nnum_results = 5\n\nsearch_results = text_rag.search(query, k=num_results)\n\nprint(\"This was the retrieved Context\")\nfor i, r in enumerate(search_results):\n    print(f\"{'==='*50}\")\n    print(f\"\\n\\nChunk {i+1}:\")\n    print(f\"Text: {r['text']}\")\n    print(f\"Chunk Index: {r['chunk_index']}\")\n    print(f\"Document Name: {r['document_name']}\")\n    print(f\"\\n\\n{'==='*50}\")\n</pre> query = \"what is colpali ?\" num_results = 5  search_results = text_rag.search(query, k=num_results)  print(\"This was the retrieved Context\") for i, r in enumerate(search_results):     print(f\"{'==='*50}\")     print(f\"\\n\\nChunk {i+1}:\")     print(f\"Text: {r['text']}\")     print(f\"Chunk Index: {r['chunk_index']}\")     print(f\"Document Name: {r['document_name']}\")     print(f\"\\n\\n{'==='*50}\") In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, Markdown, Latex\n\ncontext = \"\\n\".join([r[\"text\"] for r in search_results])\nresponse = llm.query(\n    context=context,\n    system_prompt=\"Given the below information answer the questions\",\n    query=query,\n)\n\n\ndisplay(Markdown(response))\n</pre> from IPython.display import display, Markdown, Latex  context = \"\\n\".join([r[\"text\"] for r in search_results]) response = llm.query(     context=context,     system_prompt=\"Given the below information answer the questions\",     query=query, )   display(Markdown(response)) In\u00a0[\u00a0]: Copied! <pre>%cd examples\n!python textDemo.py --share\n</pre> %cd examples !python textDemo.py --share"},{"location":"simpleRAG/#simple-rag-using-varag","title":"simple RAG using VARAG\u00b6","text":"<p>Requirement to RUN this notebook - CPU or T4(if using OCR and need fast OCR)</p>"},{"location":"simpleRAG/#run-gradio-demo","title":"Run Gradio Demo\u00b6","text":""},{"location":"visionRAG/","title":"Vision RAG","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/adithya-s-k/VARAG\n%cd VARAG\n%pwd\n</pre> !git clone https://github.com/adithya-s-k/VARAG %cd VARAG %pwd In\u00a0[\u00a0]: Copied! <pre>!apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils\n</pre> !apt-get update &amp;&amp; apt-get install -y &amp;&amp; apt-get install -y poppler-utils In\u00a0[\u00a0]: Copied! <pre>%pip install -e .\n</pre> %pip install -e . In\u00a0[\u00a0]: Copied! <pre>from sentence_transformers import SentenceTransformer\nfrom varag.rag import VisionRAG\nfrom varag.vlms import OpenAI\nimport lancedb\nimport os\nfrom dotenv import load_dotenv\n\nos.environ[\"OPENAI_API_KEY\"] = \"api-key\"\n\nload_dotenv()\n</pre> from sentence_transformers import SentenceTransformer from varag.rag import VisionRAG from varag.vlms import OpenAI import lancedb import os from dotenv import load_dotenv  os.environ[\"OPENAI_API_KEY\"] = \"api-key\"  load_dotenv() In\u00a0[\u00a0]: Copied! <pre>shared_db = lancedb.connect(\"~/shared_rag_db\")\n\n# Initialize VisionRAG and VLM\nembedding_model = SentenceTransformer(\"jinaai/jina-clip-v1\", trust_remote_code=True)\n\nvision_rag = VisionRAG(\n    image_embedding_model=embedding_model,\n    db=shared_db,\n    table_name=\"visionDemo\",\n)\n\nvlm = OpenAI()\n</pre> shared_db = lancedb.connect(\"~/shared_rag_db\")  # Initialize VisionRAG and VLM embedding_model = SentenceTransformer(\"jinaai/jina-clip-v1\", trust_remote_code=True)  vision_rag = VisionRAG(     image_embedding_model=embedding_model,     db=shared_db,     table_name=\"visionDemo\", )  vlm = OpenAI() In\u00a0[\u00a0]: Copied! <pre>vision_rag.index(\n        \"./examples/data\", \n        overwrite=False, \n        recursive=False, \n        verbose=True\n    )\n</pre> vision_rag.index(         \"./examples/data\",          overwrite=False,          recursive=False,          verbose=True     ) In\u00a0[\u00a0]: Copied! <pre>query = \"What is Colpali\"\nnum_results = 5\n\nresults = vision_rag.search(query, k=5)\n\nimages = [result[\"image\"] for result in results]\n\n# Display the images\nfor i, img in enumerate(images, 1):\n    print(f\"Image {i}:\")\n    display(img)\n</pre> query = \"What is Colpali\" num_results = 5  results = vision_rag.search(query, k=5)  images = [result[\"image\"] for result in results]  # Display the images for i, img in enumerate(images, 1):     print(f\"Image {i}:\")     display(img) In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, Markdown\n\n\nresponse = vlm.query(query, images, max_tokens=1000)\n\n\ndisplay(Markdown(response))\n</pre> from IPython.display import display, Markdown   response = vlm.query(query, images, max_tokens=1000)   display(Markdown(response)) In\u00a0[\u00a0]: Copied! <pre>%cd examples\n!python visionDemo.py --share\n</pre> %cd examples !python visionDemo.py --share"},{"location":"visionRAG/#vision-rag-using-varag","title":"vision RAG using VARAG\u00b6","text":"<p>Requirement to RUN this notebook - Min T4 GPU</p>"},{"location":"visionRAG/#run-gradio-demo","title":"Run Gradio Demo\u00b6","text":""}]}